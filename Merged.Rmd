---
title: "R Notebook"
output: html_notebook
---
##Introduction
# As Students of the university of economics in Vienna we have to be aware of dilemmas that can face any business. Being able to judge a situation facing a company correctly and act accordingly is one of our most important tasks as future graduates. Through this course we have learned many ways to analyze a companies data and make predictions for future scenarios

#Starting this project we thought of different circumstances threatening a company and coming up proper analyzing methods and a way to predict an outcome. One of the biggest factors influencing a company's future is customer satisfaction. For a business to keep its customer's satisfaction level high or at least stable is no easy task. Our goal is to identify and understand the factors that can influence the satisfaction level. Furthermore we expanded our research in training our model to different methods to find out, whether the prediction of customer satisfaction is possible or not and if so, what method could prove to be the optimal one for future use.


###Variable Declaration
Gender: Gender of the passengers (Female, Male)
Customer Type: The customer type (Loyal customer, disloyal customer)
Age: The actual age of the passengers
Type of Travel: Purpose of the flight of the passengers (Personal Travel, Business Travel)
Class: Travel class in the plane of the passengers (Business, Eco, Eco Plus)
Flight distance: The flight distance of this journey
Inflight wifi service: Satisfaction level of the inflight wifi service (0:Not Applicable;1-5)
Departure/Arrival time convenient: Satisfaction level of Departure/Arrival time convenient
Ease of Online booking: Satisfaction level of online booking
Gate location: Satisfaction level of Gate location
Food and drink: Satisfaction level of Food and drink
Online boarding: Satisfaction level of online boarding
Seat comfort: Satisfaction level of Seat comfort
Inflight entertainment: Satisfaction level of inflight entertainment
On-board service: Satisfaction level of On-board service
Leg room service: Satisfaction level of Leg room service
Baggage handling: Satisfaction level of baggage handlin
Check-in service: Satisfaction level of Check-in service
Inflight service: Satisfaction level of inflight service
Cleanliness: Satisfaction level of Cleanliness
Departure Delay in Minutes: Minutes delayed when departure
Arrival Delay in Minutes: Minutes delayed when Arrival
Satisfaction: Airline satisfaction level(Satisfaction, neutral or dissatisfaction)


```{r}
library(caret)
library(ggplot2)
library(caTools)
```


```{r}
train <- read.csv(file = 'train.csv')
test <- read.csv(file = 'test.csv')
```

```{r}
#We downloaded the data splitted as train and test data. We thought that we should bind this two datasets for our first data discovery to see how the data we got looks like and to get a better understanding of the variables.
data <- rbind(train, test)
```

```{r}
head(data)
```

```{r}
str(data)
```

```{r}
summary(data)
```

```{r}
#After summarizing the data we see that we have 393 missing values in 'Arrival.Delay.in.Minutes' so we decided to eliminate the rows containing this missing values in both train and test (da sie nur einen sehr kleinen teil des datensatzes ausmachen).
train <- na.omit(train)
test <- na.omit(test)
```


```{r}
#To simplify the levels of satisfaction we decided to rename them from 'neutral or dissatisfied' to 'no' and 'satisfied' to 'yes'. Since we cannot detect whether a customer was neutral or not satisfied we will not miss any important information.
#test$satisfaction <-factor(test$satisfaction, levels = c("neutral or dissatisfied", "satisfied"), labels=c("no", "yes"))
#train$satisfaction <- factor(train$satisfaction, levels = c("neutral or dissatisfied", "satisfied"), labels=c("no", "yes"))

```

#To get a first look on how the different variables interact with our satisfaction variable, we created some barchart to visualize their relationship. In first graph put flight distance with our response variable. It looks like the longer the flight distance the more satisfied were the customers.

#The following variabele (Baggage.handling, Inflight.service, Inflight.entertainment, Food.and.drink, On.board.service, Seat.comfort, Cleanliness) have very similar graphs. Customers tend to be satisfied more, when the service they recieve  meet their expectations.

#Looking at the Type.of.Travel we saw that most of the customers are business travellers and tend to be more satisfied with their flight experience. One reason could be that business travellers fly more often and tend to have a different mindset than others.

```{r}
plot(data$satisfaction ~ data$Flight.Distance)
```

```{r}
plot(data$satisfaction ~ data$Baggage.handling)
```


```{r}
plot(data$satisfaction ~ data$Inflight.service)
```


```{r}
plot(data$satisfaction ~ data$Inflight.entertainment)
```


```{r}
plot(data$satisfaction ~ data$Food.and.drink)
```


```{r}
plot(data$satisfaction ~ data$Type.of.Travel)
```


```{r}
plot(data$satisfaction ~ data$On.board.service)

```


```{r}
plot(data$satisfaction ~ data$Seat.comfort)

```

```{r}
plot(data$satisfaction ~ data$Cleanliness)
```

#These boxplots deliver a better visualization of the variables. Most of these plots show a rather positive result. all the shown graphs except cleanliness and food and Drink have a mean of 4. interestingly the boxplots on board service and inflight Entertainment are identical with the mean equalling the 3rd quantile.

```{r}
boxplot(data$Flight.Distance)
```

```{r}
# 1= On board service
# 2= Seat comfort
# 3= Cleanliness
# 4= Inflight Service
# 5= Inflight Entertainment
# 6= Food and Drink
# 7= Baggage handling
boxplot(data$On.board.service, data$Seat.comfort, data$Cleanliness, data$Inflight.service, data$Inflight.entertainment, data$Food.and.drink, data$Baggage.handling)
```

```{r}
set.seed(4567)
```

#After analyzing the first inspections on our dataset we initiated our further evaluation with a generalized linear model. It fits in linear, logistic regression models and in cases where the response variable has an error distribution that is non-normal. As we can see, the final values used for the model were alpha = 0.1 and lambda = 0.004989601 and the accuracy 0.8750121. When choosing a lambda value, the goal is to find the balance between simplicity and training data fit. Can be between 0 and 1 whereas it never can be exactly 0. If the lambda value is pretty high, the risk of underfitting may occur. Else, our model will become extreme complex and overfitting is a potential risk factor. 

```{r}
model_glm <- train(satisfaction~. -X -id, data = train, method = "glmnet", trControl = trainControl(method="cv",number=10), preProcess= c("center", "scale"))
model_glm
```

#Here we have a nice visualisation of our model. 
```{r}
plot(model_glm)
```

```{r}
predicted_glm <- predict(glm_model, test)
```

```{r}
table_glm <- table(predicted_glm, test$satisfaction)
table_glm
```

```{r}
confusionMatrix_glm <- confusionMatrix(table_glm)
confusionMatrix_glm
```
#So the decision tree belongs to the tree-based classification methods. The are not just solving classification problems, but also regression problems. Important here to mention is that we just use one predictor at a node as a splitting variable. 
```{r}
model_tree <- train(
          satisfaction~. -X -id,
          train,
          method = "rpart",
          trControl = trainControl(
          method = "cv",
          number = 10,
        )
)
```
#We obtained here an accuracy of 85% which is really good.

```{r}
plot(model_tree)
```

#Now we are feeding again the prediction function with our data.
```{r}
predicted_tree <- predict(tree, test)
```

```{r}
table_tree <- table(predicted_tree, test$satisfaction)
table_tree
```

```{r}
confusionMatrix_tree <- confusionMatrix(table_tree)
```
#The k Nearest Neighbors is a supervised machine learning algorithm used for classification. In fact k-NN is not a process where a prediction model is established, instead, new data points are compared to current content and depending on the classified neighbors this new data will be assigned to a class. The k in the name of this machine learning algorithm is the number of the nearest neighbors chosen for the best classification. What we do here is cross-validating with 10 folds in order to try different parameters to determine the best k. After training on the train dataset it chose k = 9 and we got an accuracy of 0.7468.


```{r}
tcon <- trainControl(method = 'cv', number = 10, verboseIter = TRUE)
model_knn <- train(satisfaction~. -X -id,
                   train,
                   method = 'knn',
                   trControl = tcon
                   )
```


```{r}
summary(model_knn)
```


```{r}
predicted_knn <- predict(knn_model, test)
predicted_knn
```

```{r}
table_knn <- table(predicted_knn, test$satisfaction)
table_knn
```

```{r}
confusionMatrix_knn <- confusionMatrix(table_knn)
confusionMatrix_knn
```

#As already noted in the results presentation, neural network training yields random prediction models, due to randomization and greediness of training algorithm. Such greedy optimization will efficiently find a local minimum of the cost function, however, there is no guarantee that the resulting local minimum will be any close to the global minimum of the cost function.

#At the moment, instead of relying upon a single training session, we would carry out the process iteratively with predetermined timeout. This way, we can attempt to obtain a result exceeding the given accuracy threshold in feasible computing time. The best configuration should be cached and used as fallback in case the desired threshold is not reached. Important to mention is, that every time the neurons' biases are newly randomized, so that we find a model of possibly best model (within given resources, that is) without feeding the model with excessive volume of training data and thus, running into risk of overfitting.

#Further, as our visualisation has shown, some high-weighted hidden layer neurons were discarded by the output neuron. We suspect it to be an anomaly, for we do not think it makes real world sense (especially in case of flight delays, that are likely to enormously frustrate the passengers). Our prior training attempts resulted in much higher accuracy of 80-90%, yet we did not investigate the results closely to conclude it as the reason. However, we expect that adding one hidden layer may be a remedy to this problem.

```{r}
model_nnet = train(satisfaction~. -X -id, data = train, method = "nnet", trControl = trainControl(method="cv",number=10))
```

```{r}
model_nnet
```


```{r}
summary(model_nnet)
```


```{r}
predicted_nnet <- predict(model_nnet, test)
predicted_nnet
```


```{r}
table_nnet <- table(predicted_nnet, test$satisfaction)
table_nnet
```

```{r}
confusionMatrix_nnet <- confusionMatrix(table_nnet)
```
#A support vector machine divides a set of data into classes in such a way that the broadest possible area remains free of data.The support vector machine is a so called large margin classifier. The SVM does this by finding hyper-planes to divide the data into classes.This classes are used to predict.We are using the methos "svmlinear" method calculates predictions by looking in which class the new data contains. SvmLinear builds linear hyper planes for this. Other svm methods can use other geometrical options. We used the caret package cross validation to divide the dataset into 10 folds to train the model 10 different times with the different folds.CV uses 9 of the 10 fold to create new train sets and evualate the submodels with new unknown folds. We excluded the variables "X" and "id" because they are not helpfull for our prediction model. SVM has difficults to handle missing values, but we ommited the missing values so this will be no problem.

```{r}
svm_tr_control = trainControl(method = "repeatedcv", number = 10,verboseIter = TRUE)
model_svm <- train(satisfaction ~ . -X -id, train[1:5000, ], method = "svmLinear", trControl = svm_tr_control, na.action = na.pass, metric = 'Accuracy')
```
#The training with the full dataset was a huge challenge for our hardware ressources but we found out, that we do not need to train the model with the full dataset. A small excerpt of the original dataset delivers similarly good results.
#We can see that the accuracy of our SVM Model is high, this is good for balanced dataset like our Customer airline satisfaction data. If the Dataset would not be so balanced we should use the Kappa value. For example a dataset with about 70 neutral or unsatisfied an 30% satisfied. Kappa is the normalized accuracy.
```{r}
model_svm
```

```{r}
summary(model.svm)
```


```{r}
predicted_svm <- predict(model_svm, test)
```


```{r}
table_svm <- table(predicted_svm, test$satisfaction)
table_svm
```

```{r}
confusionMatrixSVM <- confusionMatrix(table_svm)
```
#The Dataset is relative balanced so we can use the accuracy as measurement for the perfomance of our models. The SVM Model reaches a accuracy about 87%, with only 5000rows of the data (less than 5% of our dataset). It is also to mention that the sensitivity of our model (true positive rate) is nearly 91%, so the chance that a predicted satisfied customer is realy satisfied is quite high.

```{r}

model_nb <- train(
                satisfaction ~. -X -id,
                train,
                method = "naive_bayes",
                trControl = trainControl(
                    method = "cv",
                    number = 10,
                  )
              )

```


```{r}
summary(model_nb)
```


```{r}
predicted_nb <- predict(model_nb, test, 
                type = "raw")
predicted_nb
```

```{r}
table_nb <- table(predicted_nb, test$satisfaction)
table_nb
```

```{r}
confusionMatrix_nb <- confusionMatrix(table_nb)
```
#As we wrote before we fear that our decision tree overfitted on the training data so as a countermeasure we tried to use the randomforest model 'ranger'(for which we heard that it is faster that the 'randomForest') from the caret package. A random forest has a lower risk of overfitting due to the fact that it generates more decision trees by only considering a random subset of variables at each step. This gave us surprisingly good results given the fact that the classifiers in the dataset are pretty evenly distributed and the accuracy of the random forest is around 0.96. We learned that this is one of the most powerful classification models but we didn't expect such a performance using only the default parameters.

#As already mentioned a random forest model generates multiple trees using smaller subsets of the variables so that every tree in the forest diverse from the others. Generating this diversity makes random forests so powerful in predicting as the smaller trees reflect some subtle outcomes that one tree is not able to. After the smaller trees have been generated when we want to classify a row of variables for example this variables will run through all trees in the model and the outcome for each tree will be predicted and be accounted for each step. The last step for classification is then to look which classifier has gotten the most votes in this process and then the random forest will classify based on the most voted outcome (this principles are called ensembled methods).

#To be honest we were not so sure how to tune the paramaters since we have read that more knowledge is required, we did not control gridding the forest. Another problem that occurs is that the model which took most of the runtime (around 1,5 hours). So we had no time to test different approaches because we probably would have increased the runtime and then would have spent too much time on waiting.

```{r}
model_forest <- train(
            satisfaction~. -X -id,
            data = train,
            method = 'ranger',
            trControl = tcon
)
```

```{r}
model_forest
```


```{r}
predicted_forest <- predict(model_forest, test)
predicted_forest
```

```{r}
table_forest <- table(predicted_forest, test$satisfaction)
table_forest
```

```{r}
confusionMatrix_forest <- confusionMatrix(table_forest)
```


#Here we are iterating through the confusionMatrixModels in a pretty straightforward way.
```{r}
list_of_tables <- list(table_knn, table_nnet, tab_svm, table_glm, table_tree, table_nb, table_forest)
list_of_pred <- list(predicted_knn, predicted_nnet, predicted_svm, predicted_logistic, predicted_tree, predicted_nb)
factor_satisfaction <- factor(test$satisfaction)
confusionMatrixModels <- list()


confusionMatrixList <- function() {
    for (j in list_of_tables) {
       confusionMatrixModels <- confusionMatrix(j)
      print(confusionMatrixModels, digits = 8)
    }
}


```

```{r}
confusionMatrixList()
```





#Next we are plotting ROC curves for each model.  
```{r}
colAUC(as.numeric(pred.nb),test$satisfaction, plotROC= TRUE )
```


```{r}
colAUC(as.numeric(predicted_knn),test$satisfaction, plotROC= TRUE )
```


```{r}
colAUC(as.numeric(predicted_logistic),test$satisfaction, plotROC= TRUE )
```


```{r}
colAUC(as.numeric(predicted_tree),test$satisfaction, plotROC= TRUE )
```


```{r}
colAUC(as.numeric(pred.svm),test$satisfaction, plotROC= TRUE )
```

```{r}
colAUC(as.numeric(pred_nnet),test$satisfaction, plotROC= TRUE )
```

```{r}
colAUC(as.numeric(predicted_forest),test$satisfaction, plotROC= TRUE )
```



#Here we are preparing the variables which belong to a specific parameter in each model: Accuracy
```{r}
forest_accuracy <- confusionMatrixForest$overall["Accuracy"]
```

```{r}
knn_accuracy <- confusionMatrixKNN$overall["Accuracy"]
```

```{r}
logistic_accuracy <- confusionMatrixLogistic$overall["Accuracy"]
```

```{r}
nnet_accuracy <- confusionMatrixNNET$overall["Accuracy"]
```
#We can already detect that our neural net scored pretty bad in comparison to our other models.

```{r}
svm_accuracy <- confusionMatrixSVM$overall["Accuracy"]
```

```{r}
tree_accuracy <- confusionMatrixTree$overall["Accuracy"]
```

```{r}
nv_accuracy <- confusionMatrixNV$overall["Accuracy"]
```

```{r}
list_of_matrixes <- list(forest_accuracy, knn_accuracy, logistic_accuracy, nnet_accuracy, svm_accuracy, tree_accuracy, nv_accuracy)
```

```{r}
as.numeric(list_of_matrixes)
```

```{r}
dataOfModels <- data.frame(
  models=c("Forest","KNN","Logistic","NNET","SVM","Tree", "NB") ,  
  accuracy=as.numeric(list_of_matrixes)
  )
```


```{r}
ggplot(dataOfModels, aes(x=models, y=accuracy)) + 
  geom_bar(stat = "identity", color="blue", fill=rgb(0.1,0.4,0.5,0.7))
```
#As plotted in the barchart above, our forest model scored the best with an accuracy of almost 1. Unfortunately our neural net is nearby 50% (which is the worst score this model can have). The other models accomplished an average score.


```{r}
ls()
```

```{r}
svm_importance <- varImp(model_svm)
plot(naive_bayes_importance)
```
#As shown in the Feature Selection Graph, if we where to counsel a company on their business model we would advise it to prioritize their online boarding service. Hence, focusing on expanding either similar features or improving on their already existing service. Even Marketing strategies could build on the good impression online boarding makes.
#The following factors are in descending priority for customer satisfaction: The flight class does seem to play significant role. Inflight entertainment could also yield the base for various marketing strategies due to its high significance. Business travellers on average are more satisfied than personal travellers.
#Ease.of.Online.booking:
#At first glance the ease of online booking does not seems relevant compared to other observations. However, this could easily be a survivor’s bias, since dissatisfied customers would not even book a flight to begin with and are therefore not included in the statistic. Saying that online booking is irrelevant is ignorant and might even have negative effects if neglected.



```{r}
testnewsatisfaction<-function (var1="Cleanliness",var2=NULL, var3=NULL, var4=NULL, var5=NULL, numCustomers=10, newvsatisfaction=5, model=model_nb){
set.seed(1500)
shuffle <- sample(nrow(test))
testCustomer <- test[shuffle[1:numCustomers],]
print(testCustomer)
for (i in 1:nrow(testCustomer)) {
  testCustomer[i,var1]<-newvsatisfaction
  testCustomer[i,var2]<-newvsatisfaction
  testCustomer[i,var3]<-newvsatisfaction
  testCustomer[i,var4]<-newvsatisfaction
  testCustomer[i,var5]<-newvsatisfaction
}
pcnew <-predict(model, testCustomer, 
                type = "raw")
testCustomer$satisfactionNew1<-pcnew
return(testCustomer)
}
testnewsatisfaction(var1="Inflight.service", var2="Cleanliness")
```
#Back to our origin research question we created a function which applies our models in order to evaluate if one or even more answers are important for our target variable (satisfaction)

#First question: What factors are highly correlated to a satisfied passenger? 
#Answer: Online-Boarding, Class, Inflight Entertainment  

#Second quesiton: How good can you predict passenger satisfaction?
#Answer: Applying the random forest model for our prediction we obtained bright results.

#Roles of the Team Members:

#h1607903 Christian Shehata: Training of GLM and Decision Tree Model and Team coordinator. Set up of Github repository: https://github.com/christianshehata/passenger-satisfaction.
#h11802161 Markus Bekhit: Training of Naive Bayes Model, lead presenter and feature selection.
#h1610007 Lata Wojciech: Training of Artifical Neural Network and responsible for data selection.
#h11777005 Mustafa Shamroukh: First inspection of data and presenter.
#h11705087 Endrit Halili: Training of KNN, Random Forest Models and feature selection.
#h11706783 Dario Kitzing: Training of Support Vector Machine, practical testing of the model and feature selection.








